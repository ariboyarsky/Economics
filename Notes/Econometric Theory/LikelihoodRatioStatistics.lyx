#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\begin_modules
theorems-ams-bytype
theorems-ams-extended-bytype
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Primer on Likelihood Ratio Tests
\end_layout

\begin_layout Author
Ariel Boyarsky
\end_layout

\begin_layout Section
Likelihood Ratio Tests
\end_layout

\begin_layout Standard
Suppose that we have data,
\begin_inset Formula 
\[
X\sim P_{\theta_{0}}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $P_{\theta_{0}}$
\end_inset

is some distribution parameterized by 
\begin_inset Formula $\theta_{0}$
\end_inset

.
 Given some estimate of the parameters 
\begin_inset Formula $\hat{\theta}$
\end_inset

 we want to test the likelihood that we see 
\begin_inset Formula $X$
\end_inset

 conditional on 
\begin_inset Formula $\theta_{0}=\hat{\theta}$
\end_inset

.
 That is we are interested in the test,
\begin_inset Formula 
\[
H_{0}:\theta_{0}\in\Theta_{0}\text{ v.s. }H_{1}:\theta\in\Theta
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\Theta_{0}$
\end_inset

 is some restriction of our parameter space, for example 
\begin_inset Formula $\Theta_{0}=\left\{ \theta:\theta_{i}=0\text{ for all }i\right\} $
\end_inset

.
 Thus we test for some realization 
\begin_inset Formula $X$
\end_inset

 whether or not the null hypothesis likely to admit this realization.
 So we can define the likelihood ratio,
\begin_inset Formula 
\[
\lambda(x)=\frac{\sup\left\{ L(\theta;x):\theta\in\Theta_{0}\right\} }{\sup\left\{ L(\theta;x):\theta\in\Theta\right\} }
\]

\end_inset


\end_layout

\begin_layout Standard
Clearly since 
\begin_inset Formula $\Theta_{0}\subset\Theta$
\end_inset

 we have that the likelihood ratio is bounded from above by 1.
 If the ratio approaches 
\begin_inset Formula $1$
\end_inset

 this implies that the null hypothesis is 
\begin_inset Quotes eld
\end_inset

as likely
\begin_inset Quotes erd
\end_inset

 as the alternative which covers the full parameter space.
 However, if the ratio is close to 
\begin_inset Formula $0$
\end_inset

 this implies the likelihood of the null is small relative to the alternative.
 This gives us evidence to reject the null.
 Also note that we can reverse the ratio and thus the above logic if we
 wish.
 Consider testing,
\begin_inset Formula 
\[
H_{0}:\theta_{0}\in\Theta_{0}\text{ v.s. }H_{1}:\theta\in\Theta_{1}
\]

\end_inset

The critical region for some 
\begin_inset Formula $k\in(0,1)$
\end_inset

 is then given by,
\begin_inset Formula 
\[
C_{1}=\left\{ x:\lambda(x)\leq k\right\} 
\]

\end_inset

Furthermore, for some 
\begin_inset Formula $k$
\end_inset

 the test is at the significance level 
\begin_inset Formula $\alpha$
\end_inset

 if,
\begin_inset Formula 
\[
\sup\left\{ P\left(\lambda(x)\leq k;\theta\in\Theta_{0}\right)\right\} =\alpha
\]

\end_inset


\end_layout

\begin_layout Standard
Thus if we can determine the appropriate 
\begin_inset Formula $k$
\end_inset

 for a given 
\begin_inset Formula $\alpha$
\end_inset

 we can examine if the likelihood ratio likes within the critical region
 for that 
\begin_inset Formula $k.$
\end_inset

 To determine this we must look to the cdf of 
\begin_inset Formula $\lambda(X)$
\end_inset

.
 So for asymptotic inference we need to consider the asymptotic distribution
 of the likelihood ratio.
 In particular, consider the function 
\begin_inset Formula $-2\log\lambda(x)$
\end_inset

 which is decreasing and so we can equivalently write the critical region
 as,
\begin_inset Formula 
\[
C_{1}=\left\{ x:\Lambda(x)\geq c\right\} =\left\{ x:-2\log\lambda(x)\geq c\right\} 
\]

\end_inset


\end_layout

\begin_layout Standard
with
\begin_inset Formula 
\[
\Lambda(x)\equiv-2\log\lambda(x)=2\left[l(\hat{\theta};x)-l(\theta_{0};x)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $l\left(\cdot;x\right)$
\end_inset

 is the log-likelihood.
 Then, 
\begin_inset Formula $\Lambda(x)$
\end_inset

 is the 
\series bold
likelihood ratio statistic.
\end_layout

\begin_layout Subsection
Asymptotic Behavior
\end_layout

\begin_layout Standard
To determine the asymptotic distribution begin with a Taylor expansion of
 the log-likelihood at 
\begin_inset Formula $\theta_{0}$
\end_inset

 about 
\begin_inset Formula $\hat{\theta}$
\end_inset

,
\begin_inset Formula 
\begin{align*}
l(\theta_{0}) & =l(\hat{\theta})+\left(\hat{\theta}-\theta_{0}\right)l'(\hat{\theta})+\frac{1}{2}\left(\hat{\theta}-\theta_{0}\right)^{2}l''(\hat{\theta})+o_{P_{\theta_{0}}}(1)\\
 & =l(\hat{\theta})+\frac{1}{2}\left(\hat{\theta}-\theta_{0}\right)^{2}l''(\hat{\theta})+o_{P_{\theta_{0}}}(1)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Using the fact that 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is the maximizes the likelihood and lies within the interior of 
\begin_inset Formula $\Theta$
\end_inset

.
 So,
\begin_inset Formula 
\begin{align*}
\Lambda(x)=2\left[l(\hat{\theta})-l(\theta_{0})\right] & =-\left(\hat{\theta}-\theta_{0}\right)^{2}l''(\hat{\theta})\\
 & =\left(\hat{\theta}-\theta_{0}\right)^{2}\left[J(\hat{\theta})\right]\\
 & =\left(\hat{\theta}-\theta_{0}\right)^{2}\left[I(\theta_{0})\frac{J(\hat{\theta})}{I(\theta_{0})}\right]
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Here we use that fact that the observed information is given by,
\begin_inset Formula 
\[
J(\theta)=\frac{\partial^{2}l(\theta)}{\partial\theta^{2}}
\]

\end_inset


\end_layout

\begin_layout Standard
So that the Fisher information is given by,
\begin_inset Formula 
\[
I(\theta)=\mathbb{E}\left[J(\theta)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
And we know that,
\begin_inset Formula 
\[
\left(\hat{\theta}-\theta_{0}\right)\sqrt{I(\theta_{0})}\overset{d}{\rightarrow}N(0,1)\text{ and }\frac{J(\hat{\theta})}{I(\theta_{0})}\overset{p}{\rightarrow}1
\]

\end_inset


\end_layout

\begin_layout Standard
In particular from the proof of the asymptotic normality of the MLE we have,
\begin_inset Formula 
\begin{align*}
\sum_{i}\frac{\partial}{\partial\theta}\log f(x_{i};\theta)-J(\theta)(\hat{\theta}-\theta) & \approx0\\
\sqrt{I(\theta)}(\hat{\theta}-\theta) & \approx\left[\sum_{i}\frac{\partial}{\partial\theta}\log f(x_{i};\theta)\right]\frac{\sqrt{I(\theta)}}{J(\theta)}\\
 & =\frac{\left[\sum_{i}\frac{\partial}{\partial\theta}\log f(x_{i};\theta)\right]}{\sqrt{I(\theta)}}\left(\frac{J(\theta)}{I(\theta)}\right)^{-1}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
And we have that 
\begin_inset Formula $\frac{\left[\sum_{i}\frac{\partial}{\partial\theta}\log f(x_{i};\theta)\right]}{\sqrt{I(\theta)}}\overset{d}{\rightarrow}N(0,1)$
\end_inset

 and 
\begin_inset Formula $\frac{J(\theta)}{I(\theta)}\overset{p}{\rightarrow}1$
\end_inset

 and so the first result follows by Slutsky.
 Note that the first occurs because 
\begin_inset Formula $\left[\sum_{i}\frac{\partial}{\partial\theta}\log f(x_{i};\theta)\right]$
\end_inset

 is simply the score function (derivative of the log-likelihood) and so
 it admits asymptotic normality in the MLE.
 Also,
\begin_inset Formula 
\[
\frac{J(\theta)}{I(\theta)}=\frac{-\sum_{i}\frac{\partial^{2}}{\partial\theta^{2}}\log f(x_{i};\theta)}{ni(\theta)}\overset{p}{\rightarrow}\frac{\mathbb{E}\left[-\frac{\partial^{2}}{\partial\theta^{2}}\log f(x_{i};\theta)\right]}{\mathbb{E}\left[-\frac{\partial^{2}}{\partial\theta^{2}}\log f(x_{i};\theta)\right]}=1
\]

\end_inset


\end_layout

\begin_layout Standard
Using the fact the observations are a random sample such that 
\begin_inset Formula $I(\theta)=ni(\theta)$
\end_inset

.
\end_layout

\begin_layout Standard
So we have that by Slutsky's lemma we have,
\begin_inset Formula 
\[
\Lambda(x)=\left(\hat{\theta}-\theta_{0}\right)^{2}\left[I(\theta_{0})\frac{J(\hat{\theta})}{I(\theta_{0})}\right]\overset{d}{\rightarrow}\chi_{1}^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
If instead we have that 
\begin_inset Formula $H_{0}$
\end_inset

 is a composite null, that is 
\begin_inset Formula $\Theta_{0}$
\end_inset

 is a linear subspace with dimension greater than 1 so that the null and
 alternate hypothesis fully specify the distribution.
 Then it turns out that,
\begin_inset Formula 
\[
\Lambda(x)\overset{d}{\rightarrow}\chi_{k-l}^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $k=\dim\Theta$
\end_inset

 and 
\begin_inset Formula $l=\dim\Theta_{0}$
\end_inset

.
 This happens because then the quadratic approximation yields a sum of 
\begin_inset Formula $k-l$
\end_inset

 square normals.
\end_layout

\begin_layout Example
\begin_inset Argument 1
status open

\begin_layout Plain Layout
One-sample t-test
\end_layout

\end_inset

 Suppose we want to test,
\begin_inset Formula 
\[
H_{0}:\theta=\theta_{0}
\]

\end_inset


\end_layout

\begin_layout Example
where 
\begin_inset Formula $\theta$
\end_inset

 is the mean of a normal distribution with unknown variance.
\begin_inset Formula 
\begin{align*}
\Theta & =\left\{ \left(\theta,\sigma^{2}\right):\theta\in\mathbb{R},\sigma^{2}\in\mathbb{R}^{+}\right\} \\
\Theta_{0} & =\left\{ \left(\theta,\sigma^{2}\right):\theta=\theta_{0},\sigma^{2}\in\mathbb{R}^{+}\right\} 
\end{align*}

\end_inset


\end_layout

\begin_layout Example
Then the density is given by,
\begin_inset Formula 
\[
f(x;\theta,\sigma^{2})=\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{1}{2\sigma^{2}}\left(x-\theta\right)^{2}\right)
\]

\end_inset


\end_layout

\begin_layout Example
Which gives a likelihood function,
\begin_inset Formula 
\[
L(\theta,\sigma^{2};x)=\prod_{i}f(x_{i};\theta,\sigma^{2})=\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left(-\frac{1}{2\sigma^{2}}\sum_{i}\left(x_{i}-\theta\right)^{2}\right)
\]

\end_inset


\end_layout

\begin_layout Example
The log-likelihood for the null is then,
\begin_inset Formula 
\[
l(\theta_{0},\sigma^{2};x)=-\frac{n}{2}\log\left(2\pi\sigma^{2}\right)-\frac{1}{2\sigma^{2}}\sum_{i}\left(x_{i}-\theta_{0}\right)^{2}
\]

\end_inset


\end_layout

\begin_layout Example
The FOC is,
\begin_inset Formula 
\begin{align*}
\frac{\partial l}{\partial\sigma^{2}} & =-\frac{n}{2\sigma^{2}}+\frac{1}{2\sigma^{4}}\sum_{i}\left(x_{i}-\theta_{0}\right)^{2}\\
\implies\sigma^{2} & =\frac{1}{n}\sum_{i=1}^{n}\left(x_{i}-\theta_{0}\right)^{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Example
Plugging this back in we get that,
\begin_inset Formula 
\begin{align*}
\sup L(\theta_{0},\sigma^{2};x) & =\left(2\pi\frac{1}{n}\sum_{i=1}^{n}\left(x_{i}-\theta_{0}\right)^{2}\right)^{-n/2}\exp\left(-\frac{1}{2\frac{1}{n}\sum_{i=1}^{n}\left(x_{i}-\theta_{0}\right)^{2}}\sum_{i}\left(x_{i}-\theta\right)^{2}\right)\\
 & =\left(\frac{2\pi}{n}\sum_{i=1}^{n}\left(x_{i}-\theta_{0}\right)^{2}\right)^{-n/2}\exp\left(-\frac{n}{2}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Example
Then using the fact that 
\begin_inset Formula $\bar{x}$
\end_inset

 is the MLE in this setting, we know,
\begin_inset Formula 
\[
\sup L(\theta,\sigma^{2};x)=\left(\frac{2\pi}{n}\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\right)^{-n/2}\exp\left(-\frac{n}{2}\right)
\]

\end_inset


\end_layout

\begin_layout Example
So,
\begin_inset Formula 
\[
\lambda(x)=\left(\frac{\frac{2\pi}{n}\sum_{i=1}^{n}\left(x_{i}-\theta_{0}\right)^{2}}{\frac{2\pi}{n}\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}\right)^{-n/2}
\]

\end_inset


\end_layout

\begin_layout Example
With a little algebra we can show this is equivalent to the usual one-sample
 t-statistic.
 Begin by noticing,
\begin_inset Formula 
\begin{align*}
\sum_{i=1}^{n}\left(x_{i}-\theta_{0}\right)^{2} & =\sum_{i=1}^{n}\left(\left(x_{i}-\bar{x}\right)+\left(\bar{x}-\theta_{0}\right)\right)^{2}\\
 & =\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+2\left(\bar{x}-\theta_{0}\right)\left(x_{i}-\bar{x}\right)+\left(\bar{x}-\theta_{0}\right)^{2}\\
 & =n\left(\bar{x}-\theta_{0}\right)^{2}+\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(\left(x_{i}-\bar{x}\right)+2\left(\bar{x}-\theta_{0}\right)\right)\\
 & =n\left(\bar{x}-\theta_{0}\right)^{2}+\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(x_{i}+\bar{x}-2\theta_{0}\right)\\
 & =n\left(\bar{x}-\theta_{0}\right)^{2}+\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Example
Then,
\begin_inset Formula 
\begin{align*}
\lambda(x) & =\left(\frac{\frac{2\pi}{n}\left(n\left(\bar{x}-\theta_{0}\right)^{2}+\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\right)}{\frac{2\pi}{n}\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}\right)^{-n/2}\\
 & =\left(\frac{n\left(\bar{x}-\theta_{0}\right)^{2}}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}+1\right)^{-n/2}
\end{align*}

\end_inset


\end_layout

\begin_layout Example
Thus, if the critical region is given by,
\begin_inset Formula 
\[
C_{1}=\left\{ x:\lambda(x)\leq k\right\} 
\]

\end_inset


\end_layout

\begin_layout Example
So it is clear that the above is a function of,
\begin_inset Formula 
\[
\frac{\left|\bar{x}-\theta_{0}\right|}{\sqrt{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}}
\]

\end_inset


\end_layout

\begin_layout Example
Note that this is very close to the t-statistic,
\begin_inset Formula 
\[
\frac{\bar{x}-\theta}{S/\sqrt{n}}\sim t(n-1)
\]

\end_inset


\end_layout

\begin_layout Example
So we can also write the critical region as,
\begin_inset Formula 
\[
C_{1}=\left\{ x:\frac{\left|\bar{x}-\theta\right|}{S/\sqrt{n}}\geq c\right\} 
\]

\end_inset


\end_layout

\begin_layout Example
That is there is a correspondence between the rejection regions of the likelihoo
d ratio test and the one-sample t statistic.
\end_layout

\begin_layout Example
In particular define,
\begin_inset Formula 
\[
t=\frac{\sqrt{n}\left|\bar{x}-\theta\right|}{\sqrt{\frac{1}{n}\sum_{i}\left(x_{i}-\bar{x}\right)^{2}}}
\]

\end_inset


\begin_inset Formula 
\begin{align*}
\lambda(x) & =\left(\frac{n\left(\bar{x}-\theta_{0}\right)^{2}}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}+1\right)^{-n/2}\\
 & =\left(\sqrt{n}t^{2}+1\right)^{-n/2}
\end{align*}

\end_inset


\end_layout

\begin_layout Example
Hence establishing an equivalence between the two tests.
\end_layout

\begin_layout Subsection
Asymptotic Power
\end_layout

\begin_layout Standard
Consider the power function for a set critical value 
\begin_inset Formula $\chi_{k-l,\alpha}^{2}$
\end_inset

,
\begin_inset Formula 
\begin{align*}
\pi_{n}(\theta+\frac{h}{\sqrt{n}}) & =P_{\theta+h/\sqrt{n}}\left(\Lambda(x)>\chi_{k-l,\alpha}^{2}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Section
Neyman-Pearson Lemma
\end_layout

\begin_layout Standard
Neyman and Pearson developed likelihood over a series of papers to unify
 various hypothesis tests.
 In particular, Neyman and Pearson (1933) shows that the likelihood ratio
 test is most powerful.
 This result is the Neyman-Pearson lemma.
\end_layout

\begin_layout Theorem
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Neyman-Pearson Lemma
\end_layout

\end_inset

 Suppose 
\begin_inset Formula $\Omega_{0}=\left\{ \theta_{0}\right\} $
\end_inset

 and 
\begin_inset Formula $\Omega_{1}=\left\{ \theta_{1}\right\} $
\end_inset

 are both simple and 
\begin_inset Formula $P_{\theta_{0}}$
\end_inset

 and 
\begin_inset Formula $P_{\theta_{1}}$
\end_inset

have densities 
\begin_inset Formula $p_{\theta_{0}}$
\end_inset

and 
\begin_inset Formula $p_{\theta_{1}}$
\end_inset

with respect to 
\begin_inset Formula $\mu$
\end_inset

.
 Then for any 
\begin_inset Formula $\alpha\in(0,1)$
\end_inset

,
\end_layout

\begin_deeper
\begin_layout Enumerate
(Existence) there exists a possible randomized test,
\begin_inset Formula 
\[
\phi:\mathcal{X\to}[0,1]
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $\mathbb{E}_{\theta_{0}}\left[\phi(x)\right]=\alpha$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\phi(x)=\begin{cases}
1 & p_{1}(x)>k\cdot p_{0}(x)\\
0 & p_{1}(x)<k\cdot p_{0}(x)
\end{cases}$
\end_inset

 for some 
\begin_inset Formula $k\equiv k(\alpha)$
\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
(Optimality) Any test 
\begin_inset Formula $\phi$
\end_inset

 that satisfies both conditions of (1) is 
\series bold
most powerful: 
\series default
\shape italic
If 
\begin_inset Formula $\phi'$
\end_inset

 is any other test with 
\begin_inset Formula $\mathbb{E}_{\theta_{0}}\left[\phi'(x)\right]\leq\alpha$
\end_inset

 then 
\begin_inset Formula $\mathbb{E}_{\theta_{1}}[\phi'(x)]\leq\mathbb{E}_{\theta_{1}}[\phi(x)]$
\end_inset


\end_layout

\begin_layout Enumerate
(Uniqueness) Any test that is most powerful must satisfy (1b) for some 
\begin_inset Formula $k=k(\alpha)$
\end_inset

 almost everywhere for 
\begin_inset Formula $\mu$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Remark
The theorem states that the likelihood ratio test which rejects the null
 whenever the ratio is greater than some 
\begin_inset Formula $k$
\end_inset

 such that rejection occurs with probability 
\begin_inset Formula $\alpha$
\end_inset

 then this test is most powerful.
\end_layout

\end_body
\end_document
